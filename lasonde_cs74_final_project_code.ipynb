{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd14b8a",
   "metadata": {},
   "source": [
    "## COSC 74/274: Machine Learning and Statistical Data Analysis\n",
    "\n",
    "## Spring 2023 Class Final Project\n",
    "\n",
    "## Katherine Lasonde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb56a7",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The goal of the course project is to implement machine learning models and concepts covered\n",
    "in this course for a real-world dataset. The project will utilize the Amazon product review dataset and focus on binary classification, multi-class classification, and clustering approaches to analyze and categorize product reviews. All code must be implemented in Python and all models must use the Scikit Learn toolkit - https://scikit-learn.org/stable/index.html. You are not allowed to use other toolkits, such as NLTK or transformer network architectures, for your project results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81daf11",
   "metadata": {},
   "source": [
    "Here are examples of some useful Scikit modules:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f2ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497b947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "training_data = pd.read_csv('Training.csv')\n",
    "testing_data = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4543daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cutoff is not an input to the model, but to the experiment.\n",
    "def threshold_label(rating, threshold=1):\n",
    "    # all samples with a rating <= cutoff will have label 0\n",
    "    class_label = 0\n",
    "\n",
    "    # all samples with a rating > threshold have label 1.\n",
    "    if rating > threshold:\n",
    "        class_label = 1\n",
    "    \n",
    "    return class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4941dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to parse data\n",
    "training_data['combined_text'] = (training_data['reviewText'].fillna('') + ' ' +  (training_data['summary'].fillna('')))\n",
    "testing_data['combined_text'] = (testing_data['reviewText'].fillna('') + ' ' +  (testing_data['summary'].fillna('')))\n",
    "\n",
    "# Concatenate data\n",
    "all_text = training_data['combined_text'].tolist() + testing_data['combined_text'].tolist()\n",
    "train_size = len(training_data) # For indexing purposes\n",
    "\n",
    "# Fit the vectorizer! for both train and test data\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', strip_accents='ascii', max_df=0.85, max_features=4000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(all_text)\n",
    "tfidf_matrix_train = tfidf_matrix.toarray()[:train_size]\n",
    "tfidf_matrix_test = tfidf_matrix.toarray()[train_size:]\n",
    "\n",
    "# Create a data frame for it\n",
    "X_train = pd.DataFrame(tfidf_matrix_train, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Use a lambda function to apply to ea value fo the training data to either 0 or 1\n",
    "y_train = training_data['overall'].apply(lambda x: threshold_label(x,2)) # Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6109e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with the test data\n",
    "X_test = pd.DataFrame(tfidf_matrix_test, columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8e3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross validation object with 5-folds\n",
    "num_folds = 5\n",
    "cross_validation = StratifiedKFold(n_splits=num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b84b6",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "In this task, you have to develop binary classifiers to classify product reviews as good or bad. \n",
    "\n",
    "The cutoff of ‘goodness’ will be an input, i.e., you have to develop classifiers with the following cutoffs of product rating: 1,2,3,4. \n",
    "\n",
    "Note: The cutoff is not an input to the model, but to the experiment. For example, when cutoff=3, all samples with a rating <= 3 will have label 0, and all samples with a rating > 3 have label 1. \n",
    "- Report the performance of at least three different classifiers for each of the four cutoffs. \n",
    "- Perform cross-validation for hyperparameter tuning. \n",
    "\n",
    "Your report should describe why certain model parameters help or hurt the model performance. For each classifier, you should report in your report the confusion matrix, ROC, AUC, macro F1 score, and accuracy for the best combination of hyperparameters using 5-fold cross-validation. We will share a baseline macro F1 score for classification and at least one of your classification models must achieve at least the baseline score for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b6ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "def plot_roc_curve(false_positive_rate, true_positive_rate):\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(false_positive_rate, true_positive_rate)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.show()\n",
    "    \n",
    "# Define a function to calculate the best model and associated metrics for each cutoff\n",
    "def evaluate_performance(y_test, y_pred):  \n",
    "    num_decimal_points = 3\n",
    "\n",
    "    # Calculate 1. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate 2. Accuracy \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate 3. ROC\n",
    "    false_positive_rate, true_positive_rate, _ = roc_curve(y_test, y_pred)\n",
    "    # plot_roc_curve(false_positive_rate, true_positive_rate)\n",
    "\n",
    "    # Calculate 4. AUC\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate 5. Macro F1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(\"\\tConfusion Matrix: TP = \" + str(cm[0][0]) + \"\\tTN = \" + str(cm[1][1]) + \"\\tFP = \" + str(cm[0][1]) + \"\\tFN = \" + str(cm[1][0]))\n",
    "    print(\"\\tFPR: \" + str(round(np.array(false_positive_rate)[1], num_decimal_points)) + \" TPR: \" + str(round(np.array(true_positive_rate)[1], num_decimal_points)))\n",
    "    print(\"\\tAccuracy: \" + str(round(accuracy, num_decimal_points)))\n",
    "    print(\"\\tAUC Score: \" + str(round(auc_score,num_decimal_points)))\n",
    "    print(\"\\tF1 Score: \" + str(round(f1, num_decimal_points)))\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49af412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best fit for the data given a model type\n",
    "def find_best_fit(fcn, param, cross_validation, X_train, y_train):    \n",
    "    # Separate the test and train data with the new y-threshold\n",
    "    x_train_specific, x_test_specific, y_train_specific, y_test_specific = train_test_split(X_train, y_train, test_size = 0.15)\n",
    "        \n",
    "    # Fit the function to the training daata\n",
    "    fcn.fit(x_train_specific, y_train_specific)\n",
    "         \n",
    "    # Define grid search with cross-validation via grid search cv\n",
    "    grid_search_fcn = GridSearchCV(fcn, param_grid=param, cv=cross_validation, scoring='f1_macro')\n",
    "        \n",
    "    # Fit the grid search validation to the data\n",
    "    grid_search_fcn.fit(x_train_specific, y_train_specific)\n",
    "\n",
    "    # Predict using the best model\n",
    "    best_model = grid_search_fcn.best_estimator_\n",
    "        \n",
    "    return [best_model, x_test_specific, y_test_specific]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ff53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data for submission  \n",
    "def export_data(best_model, current_threshold):\n",
    "    y_pred_submission = best_model.predict(X_test)\n",
    "\n",
    "    # IMPORTANT: use 'id' and 'predicted' as the column names\n",
    "    test_ids = list(testing_data.index) # the 'id' column name is the index of the test samples\n",
    "    test_submission_mnb = pd.DataFrame({'id':test_ids, 'binary_split_' + str(current_threshold): y_pred_submission})\n",
    "\n",
    "    test_submission_mnb.to_csv('test_submission_mnb' + str(current_threshold) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e84840",
   "metadata": {},
   "source": [
    "# Binary classifier 1 - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280e0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First classifier, logistic regression\n",
    "logistic_regression = LogisticRegression(max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e445d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': [0.1, 0.5, 1, 5, 10]} F1: 0.7953245026178931\n",
      "Parameters: {'C': [0.01]} F1: 0.5135770099165156\n",
      "Parameters: {'C': [0.1]} F1: 0.7606102190258699\n",
      "Parameters: {'C': [0.01, 0.1]} F1: 0.7671353712636166\n",
      "Parameters: {'C': [0.01, 0.1, 1]} F1: 0.7918924056648609\n",
      "Parameters: {'C': [0.1, 1]} F1: 0.7908520171264223\n",
      "Parameters: {'C': [1]} F1: 0.7867407153513046\n",
      "Parameters: {'C': [1, 10]} F1: 0.7891742356613155\n",
      "Parameters: {'C': [0.01, 1, 10]} F1: 0.7969903922907082\n",
      "Parameters: {'C': [0.01, 0.1, 1, 10]} F1: 0.794535760835229\n",
      "Parameters: {'C': [0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10]} F1: 0.7953250025723486\n",
      "Parameters: {'C': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.6, 0.75, 0.8, 1, 2.5, 5, 7.5, 10]} F1: 0.7823624992213625\n",
      "Mean F1 score: 0.7049630870421113\n"
     ]
    }
   ],
   "source": [
    "# Test different parameters\n",
    "parameters_logistic_regression = {'C': [0.1, 0.5, 1, 5, 10]}\n",
    "parameters_logistic_regression_1 = {'C': [0.01]}\n",
    "parameters_logistic_regression_2 = {'C': [0.1]}\n",
    "parameters_logistic_regression_3 = {'C': [0.01, 0.1]}\n",
    "parameters_logistic_regression_4 = {'C': [0.01, 0.1, 1]}\n",
    "parameters_logistic_regression_5 = {'C': [0.1, 1]}\n",
    "parameters_logistic_regression_6 = {'C': [1]}\n",
    "parameters_logistic_regression_7 = {'C': [1, 10]}\n",
    "parameters_logistic_regression_8 = {'C': [0.01, 1, 10]}\n",
    "parameters_logistic_regression_9 = {'C': [0.01, 0.1, 1, 10]}\n",
    "parameters_logistic_regression_10 = {'C': [0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10]}\n",
    "parameters_logistic_regression_11 = {'C': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.5, 0.6, 0.75, 0.8, 1, 2.5, 5, 7.5, 10]}\n",
    "parameters_logistic_regression_12 = {'C': [0.01, 0.025, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 1, 2.5, 5, 7.5, 10]}\n",
    "\n",
    "lr_param_array = [parameters_logistic_regression, parameters_logistic_regression_1, parameters_logistic_regression_2, parameters_logistic_regression_3, parameters_logistic_regression_4, parameters_logistic_regression_5, parameters_logistic_regression_6, parameters_logistic_regression_7, parameters_logistic_regression_8, parameters_logistic_regression_9, parameters_logistic_regression_10, parameters_logistic_regression_11, parameters_logistic_regression_12]\n",
    "mean_f1_score = 0\n",
    "\n",
    "for i in range(len(lr_param_array)-1):\n",
    "    [best_model, X_test_lr, y_test_lr] = find_best_fit(logistic_regression, lr_param_array[i], cross_validation, X_train, y_train)\n",
    "    y_pred_lr = best_model.predict(X_test_lr)\n",
    "    f1 = f1_score(y_test_lr, y_pred_lr, average='macro')\n",
    "    mean_f1_score = mean_f1_score + f1\n",
    "    print(\"Parameters: \" + str(lr_param_array[i]) + \" F1: \" + str(f1))\n",
    "\n",
    "mean_f1_score = mean_f1_score/len(lr_param_array)\n",
    "\n",
    "print('Mean F1 score: ' + str(mean_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c692174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval the model and param for each fold!! \n",
    "print(\"Logistic Regression results: \")\n",
    "\n",
    "lr_best_models = []\n",
    "\n",
    "for current_threshold in range(4):\n",
    "    print(\"\\n Threshold number \" + str(current_threshold + 1))\n",
    "    y_train = training_data['overall'].apply(lambda x: threshold_label(x,current_threshold + 1)) # Binary\n",
    "    \n",
    "    # Now.... use the trained model & make ~predictions~ via the .predict method\n",
    "    [best_model, X_test_lr, y_test_lr] = find_best_fit(logistic_regression, parameters_logistic_regression, cross_validation, X_train, y_train)\n",
    "    \n",
    "    y_pred_lr = best_model.predict(X_test_lr)\n",
    "    \n",
    "    # Get the metrics (mainly printed except for f1)\n",
    "    f1 = evaluate_performance(y_test_lr, y_pred_lr)\n",
    "    \n",
    "    lr_best_models.append(best_model)\n",
    "\n",
    "#     export_data(best_model, current_threshold + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebc39b",
   "metadata": {},
   "source": [
    "# Binary classifier 2 - Multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the multinomial naive bayes model\n",
    "multinomial_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a bunch of the parameters for grid search\n",
    "parameters_mnb =  {'alpha': [0.001], 'fit_prior': [True]}\n",
    "parameters_mnb_1 = {'alpha': [0.001], 'fit_prior': [False]}\n",
    "parameters_mnb_2 = {'alpha': [0.01], 'fit_prior': [True]}\n",
    "parameters_mnb_3 = {'alpha': [0.01], 'fit_prior': [False]}\n",
    "parameters_mnb_4 = {'alpha': [0.1], 'fit_prior': [True]}\n",
    "parameters_mnb_5 = {'alpha': [0.1], 'fit_prior': [False]}\n",
    "parameters_mnb_6 = {'alpha': [1], 'fit_prior': [True]}\n",
    "parameters_mnb_7 = {'alpha': [1], 'fit_prior': [False]}\n",
    "parameters_mnb_8 = {'alpha': [10], 'fit_prior': [True]}\n",
    "parameters_mnb_9 = {'alpha': [10], 'fit_prior': [False]}\n",
    "parameters_mnb_10 = {'alpha': np.logspace(-1, 1, 10), 'fit_prior': [True, False]}\n",
    "parameters_mnb_11 = {'alpha': np.logspace(-3, 1, 10), 'fit_prior': [True, False]}\n",
    "parameters_mnb_12 = {'alpha': np.logspace(-3, 3, 10), 'fit_prior': [True, False]}\n",
    "parameters_mnb_13 = {'alpha': np.logspace(-2, 2, 6), 'fit_prior': [True, False]}\n",
    "\n",
    "mnb_param_array = [parameters_mnb, parameters_mnb_1, parameters_mnb_2, parameters_mnb_3, parameters_mnb_4, parameters_mnb_5, parameters_mnb_6, parameters_mnb_7, parameters_mnb_8, parameters_mnb_9, parameters_mnb_10, parameters_mnb_11, parameters_mnb_12, parameters_mnb_13]\n",
    "mean_f1_score_mnb = 0\n",
    "\n",
    "for i in range(len(mnb_param_array)):\n",
    "    [best_model, X_test_mnb, y_test_mnb] = find_best_fit(multinomial_nb, mnb_param_array[i], cross_validation, X_train, y_train)\n",
    "    y_pred_mnb = best_model.predict(X_test_mnb)\n",
    "    f1_mnb = f1_score(y_test_mnb, y_pred_mnb, average='macro')\n",
    "    mean_f1_score_mnb = mean_f1_score_mnb + f1_mnb\n",
    "    print(\"Parameters: \" + str(mnb_param_array[i]) + \" f1 : \" + str(f1_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1_score_all_mnb = mean_f1_score_mnb/len(mnb_param_array) \n",
    "print(\"The mean score for all hyperparameters for multinomial naive bayes: \" + str(mean_f1_score_all_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39244510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval the model and param for each fold!! \n",
    "print(\"Multinomial naive bayes results with the selected parameters \" + str(parameters_mnb))\n",
    "\n",
    "mnb_best_models = []\n",
    "\n",
    "for current_threshold in range(4):\n",
    "    print(\"\\n Threshold number \" + str(current_threshold + 1))\n",
    "    y_train = training_data['overall'].apply(lambda x: threshold_label(x,current_threshold + 1)) # Binary\n",
    "    \n",
    "    # Now.... use the trained model & make ~predictions~ via the .predict method\n",
    "    [best_model, X_test_mnb, y_test_mnb] = find_best_fit(multinomial_nb, parameters_mnb_11, cross_validation, X_train, y_train)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test_mnb)\n",
    "    \n",
    "    # Get the metrics (mainly printed except for f1)\n",
    "    f1 = evaluate_performance(y_test_mnb, y_pred)\n",
    "    \n",
    "    mnb_best_models.append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de764d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnb_best_models)\n",
    "\n",
    "for i in range(len(mnb_best_models)):\n",
    "    export_data(mnb_best_models[i], i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de9acc",
   "metadata": {},
   "source": [
    "# Binary classifier 3 - Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron = Perceptron(tol=1e-3, random_state=0)\n",
    "perptron_parameters = {'penalty': ['l2'], 'alpha': [0.0001], 'max_iter': [1000], 'tol': [1e-3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee632c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a bunch of parameters to see which are the best\n",
    "parameters_percep_1 = {'penalty': ['l2'], 'alpha': [0.0001], 'max_iter': [1000], 'tol': [1e-3]}\n",
    "parameters_percep_2 = {'penalty': ['l1'], 'alpha': [0.001], 'max_iter': [1000], 'tol': [1e-3]}\n",
    "parameters_percep_3 = {'penalty': ['elasticnet'], 'alpha': [0.01], 'max_iter': [1000], 'tol': [1e-3]}\n",
    "parameters_percep_4 = {'penalty': ['l2'], 'alpha': [0.0001], 'max_iter': [2000], 'tol': [1e-3]}\n",
    "parameters_percep_5 = {'penalty': ['l1'], 'alpha': [0.001], 'max_iter': [2000], 'tol': [1e-3]}\n",
    "parameters_percep_6 = {'penalty': ['elasticnet'], 'alpha': [0.01], 'max_iter': [2000], 'tol': [1e-3]}\n",
    "parameters_percep_7 = {'penalty': ['l2'], 'alpha': [0.0001], 'max_iter': [1000], 'tol': [1e-4]}\n",
    "parameters_percep_8 = {'penalty': ['l1'], 'alpha': [0.001], 'max_iter': [1000], 'tol': [1e-4]}\n",
    "parameters_percep_9 = {'penalty': ['elasticnet'], 'alpha': [0.01], 'max_iter': [1000], 'tol': [1e-4]}\n",
    "parameters_percep_10 = {'penalty': ['None'], 'max_iter': [1000], 'tol': [1e-3]}\n",
    "\n",
    "parameters_percep_array = [parameters_percep_1, parameters_percep_2, parameters_percep_3, parameters_percep_4, parameters_percep_5, parameters_percep_6, parameters_percep_7, parameters_percep_8, parameters_percep_9, parameters_percep_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1_score_percep = 0\n",
    "\n",
    "for i in range(len(parameters_percep_array)):\n",
    "    [best_model, X_test_percep, y_test_percep] = find_best_fit(perceptron, parameters_percep_array[i], cross_validation, X_train, y_train)\n",
    "    y_pred_percep = best_model.predict(X_test_percep)\n",
    "    f1_percep = f1_score(y_test_percep, y_pred_percep, average='macro')\n",
    "    mean_f1_score_percep = mean_f1_score_percep + f1_percep\n",
    "    print(\"Parameters: \" + str(parameters_percep_array[i]) + \" f1 : \" + str(f1_percep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1_score_all_percep = mean_f1_score_percep/len(parameters_percep_array) \n",
    "print(\"The mean score for all hyperparameters for perceptron: \" + str(mean_f1_score_all_percep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perceptron model with selected parameters of : \" + str())\n",
    "\n",
    "for current_threshold in range(4):\n",
    "    print(\"\\n Threshold number \" + str(current_threshold + 1))\n",
    "    y_train = training_data['overall'].apply(lambda x: threshold_label(x,current_threshold + 1)) # Binary\n",
    "    \n",
    "    # Now.... use the trained model & make ~predictions~ via the .predict method\n",
    "    [best_model_percep, X_test_percep, y_test_percep] = find_best_fit(perceptron, parameters_percep_10, cross_validation, X_train, y_train)\n",
    "    \n",
    "    y_pred_percep = best_model_percep.predict(X_test_percep)\n",
    "    \n",
    "    # Get the metrics (mainly printed except for f1)\n",
    "    f1 = evaluate_performance(y_test_percep, y_pred_percep)\n",
    "    \n",
    "    print(\"Threshold: \" + str(current_threshold+1) + ' F1: ' + str(f1))\n",
    "#     export_data(best_model, current_threshold + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd9f0",
   "metadata": {},
   "source": [
    "## Multiclass testing and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee0320",
   "metadata": {},
   "source": [
    "### Logistic regression, multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c96688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First classifier, logistic regression\n",
    "y_train = training_data['overall']\n",
    "\n",
    "# Eval the model and param for each fold!! \n",
    "print(\"Logistic Regression multiclass results: \")\n",
    "# Now.... use the trained model & make ~predictions~ via the .predict method\n",
    "[best_model_lr_multi, X_test_lr_multi, y_test_lr_multi] = find_best_fit(logistic_regression, parameters_logistic_regression, cross_validation, X_train, y_train)\n",
    "y_pred_lr_multi = best_model_lr_multi.predict(X_test_lr_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr_proba = logistic_regression.predict_proba(X_test_lr_multi)\n",
    "f1_lr = evaluate_performance_multi(y_test_lr_multi, y_pred_lr_multi, y_pred_lr_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr_multi_submission = best_model_lr_multi.predict(X_test)\n",
    "\n",
    "# IMPORTANT: use 'id' and 'predicted' as the column names\n",
    "test_ids = list(testing_data.index) # the 'id' column name is the index of the test samples\n",
    "test_submission_dt = pd.DataFrame({'id':test_ids, 'label':y_pred_lr_multi_submission})\n",
    "\n",
    "test_submission_dt.to_csv('test_submission_lr_multi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance_multi(y_test, y_pred, y_pred_lr_proba):\n",
    "    num_decimal_points = 3\n",
    "\n",
    "    # Calculate 1. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate 2. Accuracy \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate 3. ROC\n",
    "    false_positive_rate, true_positive_rate, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
    "    plot_roc_curve(false_positive_rate, true_positive_rate)\n",
    "\n",
    "    # Calculate 4. AUC\n",
    "    auc_score = roc_auc_score(y_test, y_pred_lr_proba, multi_class='ovr')\n",
    "    \n",
    "    # Calculate 5. Macro F1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(\"\\tConfusion Matrix: \\n\\t\\tTP = \" + str(cm[0][0]) + \"\\tTN = \" + str(cm[1][1]) + \"\\tFP = \" + str(cm[0][1]) + \"\\tFN = \" + str(cm[1][0]))\n",
    "    print(\"\\tFPR: \" + str(round(np.array(false_positive_rate)[1], num_decimal_points)) + \" TPR: \" + str(round(np.array(true_positive_rate)[1], num_decimal_points)))\n",
    "    print(\"\\tAccuracy: \" + str(round(accuracy, num_decimal_points)))\n",
    "    print(\"\\tAUC Score: \" + str(round(auc_score,num_decimal_points)))\n",
    "    print(\"\\tF1 Score: \" + str(round(f1, num_decimal_points)))\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d698087",
   "metadata": {},
   "source": [
    "### Multinomial bayes, multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b993bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval the model and param for each fold!! \n",
    "print(\"Multinomial Naive Bayes multiclass results: \")\n",
    "\n",
    "# Now.... use the trained model & make ~predictions~ via the .predict method\n",
    "[best_model_mnb_multi, X_test_mnb_multi, y_test_mnb_multi] = find_best_fit(multinomial_nb, parameters_mnb_11, cross_validation, X_train, y_train)\n",
    "y_pred_mnb_multi = best_model_mnb_multi.predict(X_test_mnb_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metrics (mainly printed except for f1)\n",
    "y_pred_mnb_proba = multinomial_nb.predict_proba(X_test_mnb_multi)\n",
    "f1_mnb = evaluate_performance_multi(y_test_mnb_multi, y_pred_mnb_multi, y_pred_mnb_proba)\n",
    "# export_data(best_model_mnb_multi, current_threshold + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mnb_multi_submission = best_model_mnb_multi.predict(X_test)\n",
    "\n",
    "# IMPORTANT: use 'id' and 'predicted' as the column names\n",
    "test_ids = list(testing_data.index) # the 'id' column name is the index of the test samples\n",
    "test_submission_dt = pd.DataFrame({'id':test_ids, 'label':y_pred_mnb_multi_submission})\n",
    "\n",
    "test_submission_dt.to_csv('test_submission_mnb_multi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9638e56",
   "metadata": {},
   "source": [
    "### Perceptron, multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca77a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Perceptron model\n",
    "perceptron = Perceptron(tol=1e-3, random_state=0)\n",
    "\n",
    "# Eval the model and param for each fold!! \n",
    "print(\"Perceptron multiclass results: \")\n",
    "\n",
    "# Now.... use the trained model & make ~predictions~ via the .predict method\n",
    "[best_model_percep_multi, X_test_percep_multi, y_test_percep_multi] = find_best_fit(perceptron, parameters_percep_10, cross_validation, X_train, y_train)\n",
    "y_pred_percep_multi = best_model_percep_multi.predict(X_test_percep_multi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metrics (mainly printed except for f1)\n",
    "y_pred_percep_proba = perceptron.decision_function(X_test_percep_multi) #perceptron.predict_proba(X_test_percep_multi)\n",
    "f1_percep = evaluate_performance_multi(y_test_percep_multi, y_pred_percep_multi, y_pred_percep_proba)\n",
    "# export_data(best_model, current_threshold + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_percep_multi_submission = best_model_percep_multi.predict(X_test)\n",
    "\n",
    "# IMPORTANT: use 'id' and 'predicted' as the column names\n",
    "test_ids = list(testing_data.index) # the 'id' column name is the index of the test samples\n",
    "test_submission_dt = pd.DataFrame({'id':test_ids, 'multi_split':y_pred_percep_multi_submission})\n",
    "\n",
    "test_submission_dt.to_csv('test_submission_percep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f293bd",
   "metadata": {},
   "source": [
    "### 3.3 Clustering\n",
    "In this task, you will cluster the product reviews in the test dataset. You will need to create word features from the data and use that for k-means clustering. Clustering will be done by product types, i.e., in this case, the labels will be product categories. You will use the Silhouette score and Rand index to analyze the quality of clustering. We will share a baseline silhouette score for clustering, and your model must achieve at least the baseline score for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d608d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = training_data['category']\n",
    "# Fit the vectorizer! for both train and test data\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', strip_accents='ascii', max_df=0.85, max_features=4000)\n",
    "\n",
    "# transform the deeta\n",
    "tfidf_matrix_cluster = tfidf_vectorizer.fit_transform(cluster_labels).toarray()\n",
    "\n",
    "# Create a data frame for it\n",
    "X_train_clust = pd.DataFrame(tfidf_matrix_cluster, columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f62c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cluster, X_test_cluster, y_train_cluster, y_test_cluster = train_test_split(X_train_clust, y_train, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14878aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "true_k = 5  # adjust this to the number of product categories you have\n",
    "k_means_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "k_means_model.fit=(X_train_cluster, y_train_cluster)\n",
    "y_pred_cluster_labels = model.predict(X_test_cluster)\n",
    "\n",
    "# Calculate  silhouette_score and\n",
    "silhouette_score = silhouette_score(X_test_cluster, y_pred_cluster, metric='euclidean')\n",
    "\n",
    "# Calculate rand index score\n",
    "rand_index_score = adjusted_rand_score(y_test_cluster, y_pred_cluster)\n",
    "\n",
    "print(\"Silhouette score: \", silhouette_score)\n",
    "print(\"Rand Index Score: \", rand_index_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
